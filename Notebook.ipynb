{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing all dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO) #Provide use full information\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout)) # Give messages that helps in monitoring\n",
    "from llama_index import (SimpleDirectoryReader,# To Read Unstructured data\n",
    "                         LLMPredictor,#Generating Predictions using LLM\n",
    "                         ServiceContext,#Supplies Contextual Data \n",
    "                         KnowledgeGraphIndex)#Both Construction and manipulation of of KG\n",
    "#\n",
    "from llama_index.graph_stores import SimpleGraphStore #For storing Graph data\n",
    "from llama_index.storage.storage_context import StorageContext\n",
    "from llama_index.llms import HuggingFaceInferenceAPI # Module to leverage Open source LLMs\n",
    "from langchain.embeddings import HuggingFaceInferenceAPIEmbeddings\n",
    "from llama_index.embeddings import LangchainEmbedding #For Embeddings\n",
    "from pyvis.network import Network\n",
    "from IPython.display import display, HTML\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Open Source HuggingFace Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DIPANJAN PATRA\\Desktop\\RAG_KG\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "HF_TOKEN = \"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\"\n",
    "#   LLMs\n",
    "llm = HuggingFaceInferenceAPI(\n",
    "    model_name=\"HuggingFaceH4/zephyr-7b-beta\", token=HF_TOKEN\n",
    ")\n",
    "\n",
    "\n",
    "#Embedding Model\n",
    "embed_model = LangchainEmbedding(\n",
    "  HuggingFaceInferenceAPIEmbeddings(api_key=HF_TOKEN,model_name=\"thenlper/gte-large\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Text Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10 docs\n"
     ]
    }
   ],
   "source": [
    "reader = SimpleDirectoryReader(\n",
    "    input_files=[\"itachi.pdf\"]\n",
    ")\n",
    "docs = reader.load_data()\n",
    "print(f\"Loaded {len(docs)} docs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set Up Service Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "service_context = ServiceContext.from_defaults(\n",
    "    chunk_size=256,\n",
    "    llm=llm,\n",
    "    embed_model=embed_model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Storage Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_store = SimpleGraphStore()\n",
    "storage_context = StorageContext.from_defaults(graph_store=graph_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building Knowledge Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "HfHubHTTPError",
     "evalue": "429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/HuggingFaceH4/zephyr-7b-beta (Request ID: 7cPJF6vOQQvyc3dYAuOF5)\n\nRate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\DIPANJAN PATRA\\Desktop\\RAG_KG\\.venv\\lib\\site-packages\\huggingface_hub\\utils\\_errors.py:270\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    269\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 270\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\DIPANJAN PATRA\\Desktop\\RAG_KG\\.venv\\lib\\site-packages\\requests\\models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/HuggingFaceH4/zephyr-7b-beta",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mHfHubHTTPError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m index \u001b[38;5;241m=\u001b[39m \u001b[43mKnowledgeGraphIndex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdocs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43mmax_triplets_per_chunk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43mservice_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mservice_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43mstorage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m                                          \u001b[49m\u001b[43minclude_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\DIPANJAN PATRA\\Desktop\\RAG_KG\\.venv\\lib\\site-packages\\llama_index\\indices\\base.py:106\u001b[0m, in \u001b[0;36mBaseIndex.from_documents\u001b[1;34m(cls, documents, storage_context, service_context, show_progress, **kwargs)\u001b[0m\n\u001b[0;32m     97\u001b[0m     docstore\u001b[38;5;241m.\u001b[39mset_document_hash(doc\u001b[38;5;241m.\u001b[39mget_doc_id(), doc\u001b[38;5;241m.\u001b[39mhash)\n\u001b[0;32m     99\u001b[0m nodes \u001b[38;5;241m=\u001b[39m run_transformations(\n\u001b[0;32m    100\u001b[0m     documents,  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m    101\u001b[0m     service_context\u001b[38;5;241m.\u001b[39mtransformations,\n\u001b[0;32m    102\u001b[0m     show_progress\u001b[38;5;241m=\u001b[39mshow_progress,\n\u001b[0;32m    103\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    104\u001b[0m )\n\u001b[1;32m--> 106\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\n\u001b[0;32m    107\u001b[0m     nodes\u001b[38;5;241m=\u001b[39mnodes,\n\u001b[0;32m    108\u001b[0m     storage_context\u001b[38;5;241m=\u001b[39mstorage_context,\n\u001b[0;32m    109\u001b[0m     service_context\u001b[38;5;241m=\u001b[39mservice_context,\n\u001b[0;32m    110\u001b[0m     show_progress\u001b[38;5;241m=\u001b[39mshow_progress,\n\u001b[0;32m    111\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    112\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\DIPANJAN PATRA\\Desktop\\RAG_KG\\.venv\\lib\\site-packages\\llama_index\\indices\\knowledge_graph\\base.py:81\u001b[0m, in \u001b[0;36mKnowledgeGraphIndex.__init__\u001b[1;34m(self, nodes, index_struct, service_context, storage_context, kg_triple_extract_template, max_triplets_per_chunk, include_embeddings, show_progress, max_object_length, kg_triplet_extract_fn, **kwargs)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_object_length \u001b[38;5;241m=\u001b[39m max_object_length\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_kg_triplet_extract_fn \u001b[38;5;241m=\u001b[39m kg_triplet_extract_fn\n\u001b[1;32m---> 81\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     82\u001b[0m     nodes\u001b[38;5;241m=\u001b[39mnodes,\n\u001b[0;32m     83\u001b[0m     index_struct\u001b[38;5;241m=\u001b[39mindex_struct,\n\u001b[0;32m     84\u001b[0m     service_context\u001b[38;5;241m=\u001b[39mservice_context,\n\u001b[0;32m     85\u001b[0m     storage_context\u001b[38;5;241m=\u001b[39mstorage_context,\n\u001b[0;32m     86\u001b[0m     show_progress\u001b[38;5;241m=\u001b[39mshow_progress,\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m     88\u001b[0m )\n\u001b[0;32m     90\u001b[0m \u001b[38;5;66;03m# TODO: legacy conversion - remove in next release\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m     92\u001b[0m     \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex_struct\u001b[38;5;241m.\u001b[39mtable) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraph_store, SimpleGraphStore)\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraph_store\u001b[38;5;241m.\u001b[39m_data\u001b[38;5;241m.\u001b[39mgraph_dict) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     95\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\DIPANJAN PATRA\\Desktop\\RAG_KG\\.venv\\lib\\site-packages\\llama_index\\indices\\base.py:71\u001b[0m, in \u001b[0;36mBaseIndex.__init__\u001b[1;34m(self, nodes, index_struct, storage_context, service_context, show_progress, **kwargs)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index_struct \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m nodes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m---> 71\u001b[0m     index_struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_index_from_nodes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_struct \u001b[38;5;241m=\u001b[39m index_struct\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_storage_context\u001b[38;5;241m.\u001b[39mindex_store\u001b[38;5;241m.\u001b[39madd_index_struct(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_struct)\n",
      "File \u001b[1;32mc:\\Users\\DIPANJAN PATRA\\Desktop\\RAG_KG\\.venv\\lib\\site-packages\\llama_index\\indices\\base.py:175\u001b[0m, in \u001b[0;36mBaseIndex.build_index_from_nodes\u001b[1;34m(self, nodes)\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Build the index from nodes.\"\"\"\u001b[39;00m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_docstore\u001b[38;5;241m.\u001b[39madd_documents(nodes, allow_update\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 175\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_index_from_nodes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\DIPANJAN PATRA\\Desktop\\RAG_KG\\.venv\\lib\\site-packages\\llama_index\\indices\\knowledge_graph\\base.py:167\u001b[0m, in \u001b[0;36mKnowledgeGraphIndex._build_index_from_nodes\u001b[1;34m(self, nodes)\u001b[0m\n\u001b[0;32m    163\u001b[0m nodes_with_progress \u001b[38;5;241m=\u001b[39m get_tqdm_iterable(\n\u001b[0;32m    164\u001b[0m     nodes, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_show_progress, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing nodes\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    165\u001b[0m )\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m nodes_with_progress:\n\u001b[1;32m--> 167\u001b[0m     triplets \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_extract_triplets\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetadata_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMetadataMode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLLM\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    170\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m> Extracted triplets: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtriplets\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    171\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m triplet \u001b[38;5;129;01min\u001b[39;00m triplets:\n",
      "File \u001b[1;32mc:\\Users\\DIPANJAN PATRA\\Desktop\\RAG_KG\\.venv\\lib\\site-packages\\llama_index\\indices\\knowledge_graph\\base.py:118\u001b[0m, in \u001b[0;36mKnowledgeGraphIndex._extract_triplets\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_kg_triplet_extract_fn(text)\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_llm_extract_triplets\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\DIPANJAN PATRA\\Desktop\\RAG_KG\\.venv\\lib\\site-packages\\llama_index\\indices\\knowledge_graph\\base.py:122\u001b[0m, in \u001b[0;36mKnowledgeGraphIndex._llm_extract_triplets\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_llm_extract_triplets\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Tuple[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]]:\n\u001b[0;32m    121\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Extract keywords from text.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_service_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    123\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkg_triple_extract_template\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    124\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse_triplet_response(\n\u001b[0;32m    127\u001b[0m         response, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_object_length\n\u001b[0;32m    128\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\DIPANJAN PATRA\\Desktop\\RAG_KG\\.venv\\lib\\site-packages\\llama_index\\llms\\llm.py:220\u001b[0m, in \u001b[0;36mLLM.predict\u001b[1;34m(self, prompt, **prompt_args)\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    219\u001b[0m     formatted_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_prompt(prompt, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mprompt_args)\n\u001b[1;32m--> 220\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcomplete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformatted_prompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    221\u001b[0m     output \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mtext\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse_output(output)\n",
      "File \u001b[1;32mc:\\Users\\DIPANJAN PATRA\\Desktop\\RAG_KG\\.venv\\lib\\site-packages\\llama_index\\llms\\huggingface.py:581\u001b[0m, in \u001b[0;36mHuggingFaceInferenceAPI.complete\u001b[1;34m(self, prompt, **kwargs)\u001b[0m\n\u001b[0;32m    579\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcomplete\u001b[39m(\u001b[38;5;28mself\u001b[39m, prompt: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m CompletionResponse:\n\u001b[0;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m CompletionResponse(\n\u001b[1;32m--> 581\u001b[0m         text\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sync_client\u001b[38;5;241m.\u001b[39mtext_generation(\n\u001b[0;32m    582\u001b[0m             prompt, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_new_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_output}, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs}\n\u001b[0;32m    583\u001b[0m         )\n\u001b[0;32m    584\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\DIPANJAN PATRA\\Desktop\\RAG_KG\\.venv\\lib\\site-packages\\huggingface_hub\\inference\\_client.py:1535\u001b[0m, in \u001b[0;36mInferenceClient.text_generation\u001b[1;34m(self, prompt, details, stream, model, do_sample, max_new_tokens, best_of, repetition_penalty, return_full_text, seed, stop_sequences, temperature, top_k, top_p, truncate, typical_p, watermark, decoder_input_details)\u001b[0m\n\u001b[0;32m   1514\u001b[0m         _set_as_non_tgi(model)\n\u001b[0;32m   1515\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_generation(  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m             prompt\u001b[38;5;241m=\u001b[39mprompt,\n\u001b[0;32m   1517\u001b[0m             details\u001b[38;5;241m=\u001b[39mdetails,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1533\u001b[0m             decoder_input_details\u001b[38;5;241m=\u001b[39mdecoder_input_details,\n\u001b[0;32m   1534\u001b[0m         )\n\u001b[1;32m-> 1535\u001b[0m     \u001b[43mraise_text_generation_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# Parse output\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream:\n",
      "File \u001b[1;32mc:\\Users\\DIPANJAN PATRA\\Desktop\\RAG_KG\\.venv\\lib\\site-packages\\huggingface_hub\\inference\\_text_generation.py:521\u001b[0m, in \u001b[0;36mraise_text_generation_error\u001b[1;34m(http_error)\u001b[0m\n\u001b[0;32m    518\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhttp_error\u001b[39;00m\n\u001b[0;32m    520\u001b[0m \u001b[38;5;66;03m# Otherwise, fallback to default error\u001b[39;00m\n\u001b[1;32m--> 521\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m http_error\n",
      "File \u001b[1;32mc:\\Users\\DIPANJAN PATRA\\Desktop\\RAG_KG\\.venv\\lib\\site-packages\\huggingface_hub\\inference\\_client.py:1511\u001b[0m, in \u001b[0;36mInferenceClient.text_generation\u001b[1;34m(self, prompt, details, stream, model, do_sample, max_new_tokens, best_of, repetition_penalty, return_full_text, seed, stop_sequences, temperature, top_k, top_p, truncate, typical_p, watermark, decoder_input_details)\u001b[0m\n\u001b[0;32m   1509\u001b[0m \u001b[38;5;66;03m# Handle errors separately for more precise error messages\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     bytes_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpayload\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext-generation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m   1512\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1513\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, BadRequestError) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe following `model_kwargs` are not used by the model\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e):\n",
      "File \u001b[1;32mc:\\Users\\DIPANJAN PATRA\\Desktop\\RAG_KG\\.venv\\lib\\site-packages\\huggingface_hub\\inference\\_client.py:240\u001b[0m, in \u001b[0;36mInferenceClient.post\u001b[1;34m(self, json, data, model, task, stream)\u001b[0m\n\u001b[0;32m    237\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InferenceTimeoutError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInference call timed out: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merror\u001b[39;00m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 240\u001b[0m     \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39miter_lines() \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m response\u001b[38;5;241m.\u001b[39mcontent\n\u001b[0;32m    242\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m error:\n",
      "File \u001b[1;32mc:\\Users\\DIPANJAN PATRA\\Desktop\\RAG_KG\\.venv\\lib\\site-packages\\huggingface_hub\\utils\\_errors.py:330\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m BadRequestError(message, response\u001b[38;5;241m=\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    328\u001b[0m \u001b[38;5;66;03m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001b[39;00m\n\u001b[0;32m    329\u001b[0m \u001b[38;5;66;03m# as well (request id and/or server error message)\u001b[39;00m\n\u001b[1;32m--> 330\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m HfHubHTTPError(\u001b[38;5;28mstr\u001b[39m(e), response\u001b[38;5;241m=\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mHfHubHTTPError\u001b[0m: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/HuggingFaceH4/zephyr-7b-beta (Request ID: 7cPJF6vOQQvyc3dYAuOF5)\n\nRate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate"
     ]
    }
   ],
   "source": [
    "index = KnowledgeGraphIndex.from_documents( documents=docs,\n",
    "                                           max_triplets_per_chunk=3,\n",
    "                                           service_context=service_context,\n",
    "                                           storage_context=storage_context,\n",
    "                                          include_embeddings=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Querying Knowledge Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.indices.knowledge_graph.retrievers:> Querying with idx: 5d659c69-fcfe-49b5-8c22-718c04983ec1: 7 Conclusion and Future Work\n",
      "In conclusion, our research demonstrates the eff...\n",
      "> Querying with idx: 5d659c69-fcfe-49b5-8c22-718c04983ec1: 7 Conclusion and Future Work\n",
      "In conclusion, our research demonstrates the eff...\n",
      "INFO:llama_index.indices.knowledge_graph.retrievers:> Querying with idx: 6728ec49-e7ad-4e9a-9cf6-b6e489c8e073: This paper proposes an optimized\n",
      "solar power distribution technique for micro...\n",
      "> Querying with idx: 6728ec49-e7ad-4e9a-9cf6-b6e489c8e073: This paper proposes an optimized\n",
      "solar power distribution technique for micro...\n",
      "INFO:llama_index.indices.knowledge_graph.retrievers:> Querying with idx: 135621c9-31e0-4877-82e1-3c0fb1c803be: This paper\n",
      "proposes an optimization-based approach for solar power distributi...\n",
      "> Querying with idx: 135621c9-31e0-4877-82e1-3c0fb1c803be: This paper\n",
      "proposes an optimization-based approach for solar power distributi...\n",
      "INFO:llama_index.indices.knowledge_graph.retrievers:> Querying with idx: b2b41243-b2f5-4076-9cf9-a0e48956db01: The key performance\n",
      "metric energy waste is evaluated.\n",
      "In the non-optimized me...\n",
      "> Querying with idx: b2b41243-b2f5-4076-9cf9-a0e48956db01: The key performance\n",
      "metric energy waste is evaluated.\n",
      "In the non-optimized me...\n",
      "INFO:llama_index.indices.knowledge_graph.retrievers:> Querying with idx: 2a45ed1c-fcd6-46d8-858f-dc42f02851a5: Keywords: Microgrid, solar power, optimization, energy distribution,\n",
      "sustaina...\n",
      "> Querying with idx: 2a45ed1c-fcd6-46d8-858f-dc42f02851a5: Keywords: Microgrid, solar power, optimization, energy distribution,\n",
      "sustaina...\n",
      "INFO:llama_index.indices.knowledge_graph.retrievers:> Querying with idx: 4b7c7f6e-dd76-4cef-ac1d-74a81055c5ce: The objective is to optimally distribute solar power among\n",
      "homes in a microgr...\n",
      "> Querying with idx: 4b7c7f6e-dd76-4cef-ac1d-74a81055c5ce: The objective is to optimally distribute solar power among\n",
      "homes in a microgr...\n",
      "INFO:llama_index.indices.knowledge_graph.retrievers:> Querying with idx: 5554aa71-0780-440c-95ff-3761019a078d: This highlights the importance\n",
      "of accurate predictions in achieving energy ef...\n",
      "> Querying with idx: 5554aa71-0780-440c-95ff-3761019a078d: This highlights the importance\n",
      "of accurate predictions in achieving energy ef...\n",
      "INFO:llama_index.indices.knowledge_graph.retrievers:> Querying with idx: cde655d1-1738-4b1b-bb42-1918b9f5757a: The simulation results shown in Table 2 demonstrates significant improve-\n",
      "men...\n",
      "> Querying with idx: cde655d1-1738-4b1b-bb42-1918b9f5757a: The simulation results shown in Table 2 demonstrates significant improve-\n",
      "men...\n",
      "INFO:llama_index.indices.knowledge_graph.retrievers:> Querying with idx: be4de5fd-10a5-484f-b449-4a6d599f16af: By considering factors such as predicted solar\n",
      "energy, actual solar generatio...\n",
      "> Querying with idx: be4de5fd-10a5-484f-b449-4a6d599f16af: By considering factors such as predicted solar\n",
      "energy, actual solar generatio...\n",
      "INFO:llama_index.indices.knowledge_graph.retrievers:> Querying with idx: 62a5c1ed-7ee5-4575-96e6-b5a3da6f06ee: The optimal distribution of solar power in microgrids is cru-\n",
      "cial for maximi...\n",
      "> Querying with idx: 62a5c1ed-7ee5-4575-96e6-b5a3da6f06ee: The optimal distribution of solar power in microgrids is cru-\n",
      "cial for maximi...\n",
      "Based on the provided context, the paper aims to develop an optimization model considering predicted solar energy and actual solar generation for efficient power distribution within microgrids, formulate an objective function that minimizes energy waste by optimizing the allocation of solar power based on individual load requirements, investigate the influence of the accuracy of the solar power forecasting model on reducing energy waste and improving overall system performance, and implement and evaluate the proposed solar power distribution technique through numerical simulations and analysis.\n"
     ]
    }
   ],
   "source": [
    "query = \"What the paper aims?\"\n",
    "query_engine = index.as_query_engine(include_text=True,\n",
    "                                     response_mode =\"tree_summarize\",\n",
    "                                     embedding_mode=\"hybrid\",\n",
    "                                     similarity_top_k=5,)\n",
    "#\n",
    "message_template =f\"\"\"<|system|>Please check if the following pieces of context has any mention of the  keywords provided in the Question.If not then don't know the answer, just say that you don't know.Stop there.Please donot try to make up an answer.</s>\n",
    "<|user|>\n",
    "Question: {query}\n",
    "Helpful Answer:\n",
    "</s>\"\"\"\n",
    "#\n",
    "response = query_engine.query(message_template)\n",
    "#\n",
    "print(response.response.split(\"<|assistant|>\")[-1].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graph Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Assuming `index` is your knowledge graph index\n",
    "g = index.get_networkx_graph()\n",
    "\n",
    "# Create a pyvis network\n",
    "net = Network(notebook=True,cdn_resources='remote',directed=True)\n",
    "\n",
    "# Add nodes and edges to the network\n",
    "net.from_nx(g)\n",
    "\n",
    "# Save the graph to an HTML file\n",
    "net.save_graph(\"Knowledge_graph2.html\")\n",
    "\n",
    "# # Display the graph in the notebook\n",
    "# display(HTML(\"Knowledge_graph.html\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
